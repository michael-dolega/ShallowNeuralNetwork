{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dNRaeOqJ7BM4Jq1g1sQAcIelqWNcF5lB",
      "authorship_tag": "ABX9TyMrKbYZoxTU/lrWQOq+BnR1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a1pha609/sparky/blob/main/ShallowNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk_cwvzHzF_2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "27mI7Oj_1Ngk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using simple Neural Network to solve XOR problem\n",
        "\n",
        "Author: Michael Dolega\n",
        "\n",
        "Date: Sep 19, 2025\n",
        "\n",
        "Notes:\n",
        "\n",
        "The Python code is losely modeled on a GeeksforGeeks tutorial on [backpropagation in neural networks](https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/).\n",
        "\n",
        "This code is intended as a practice exercise.\n",
        "\n",
        "Defining the logic facilitates the understanding of the NN architecture.\n",
        "\n",
        "It also solidifies the importance of activation functions, and their use in gradient calculations.\n",
        "\n"
      ],
      "metadata": {
        "id": "8geRVUXdzOHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the needed libraries.\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "x0J1xlRt1bM7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Shallow Neural Network (SNN) object class.\n",
        "\n",
        "class ShallowNeuralNetwork:\n",
        "  # Initialize by defininig input size, number of neurons in hidden layer, and output vector size.\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.wih = np.random.randn(self.input_size, self.hidden_size)\n",
        "    self.who = np.random.randn(self.hidden_size, self.output_size)\n",
        "    self.bih = np.zeros((1, self.hidden_size))\n",
        "    self.bho = np.zeros((1, self.output_size))\n",
        "\n",
        "  # Define the activation function as the softplus function.\n",
        "  def softplus(self, x):\n",
        "    return np.log(1 + np.exp(x))\n",
        "\n",
        "  def d_softplus(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "# Define the forward pass mechanism.\n",
        "  def feed_forward(self,X):\n",
        "    self.hidden_activation = np.dot(X,self.wih) + self.bih\n",
        "    self.hidden_output = self.softplus(self.hidden_activation)\n",
        "\n",
        "    self.output_activation = np.dot(self.hidden_output,self.who) + self.bho\n",
        "    self.predicted_output = self.softplus(self.output_activation)\n",
        "\n",
        "    return self.predicted_output\n",
        "# Define the back propagate mechanism.\n",
        "  def back_propagate(self, X, y, learning_rate):\n",
        "    self.output_error = y - self.predicted_output\n",
        "    self.output_delta = self.output_error * self.d_softplus(self.predicted_output)\n",
        "\n",
        "    self.hidden_error = np.dot(self.output_delta, self.who.T)\n",
        "    self.hidden_delta = self.hidden_error * self.d_softplus(self.hidden_output)\n",
        "\n",
        "    self.who += learning_rate * np.dot(self.hidden_output.T, self.output_delta)\n",
        "    self.wih += learning_rate * np.dot(X.T, self.hidden_delta)\n",
        "    self.bho += learning_rate * np.sum(self.output_delta, axis=0, keepdims=True)\n",
        "    self.bih += learning_rate * np.sum(self.hidden_delta, axis=0, keepdims=True)\n",
        "\n",
        "    return self.output_error\n",
        "\n",
        "  def train(self,X,y, epochs, learning_rate):\n",
        "    for epoch in range(epochs):\n",
        "      output = self.feed_forward(X)\n",
        "      error = self.back_propagate(X, y, learning_rate)\n",
        "      if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Error: {error}\")\n"
      ],
      "metadata": {
        "id": "arsm6ADu1jfj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1 ]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "snn = ShallowNeuralNetwork(2,10,1)\n",
        "snn.train(X,y,10000,0.1)\n",
        "\n",
        "output = snn.feed_forward(X)\n",
        "print(f\"XOR prediction output: {output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIOsYrqE70DX",
        "outputId": "e6e8294e-7ae1-44bd-bb9d-c6aa5c6ccc53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000, Error: [[-0.894721  ]\n",
            " [ 0.0166899 ]\n",
            " [ 0.40081167]\n",
            " [-0.78245339]]\n",
            "Epoch 1001/10000, Error: [[-0.01705132]\n",
            " [ 0.24389623]\n",
            " [ 0.17236687]\n",
            " [-0.01324048]]\n",
            "Epoch 2001/10000, Error: [[-0.00800802]\n",
            " [ 0.1724213 ]\n",
            " [ 0.13160413]\n",
            " [-0.00744671]]\n",
            "Epoch 3001/10000, Error: [[-0.0055135 ]\n",
            " [ 0.14419116]\n",
            " [ 0.11627282]\n",
            " [-0.00551007]]\n",
            "Epoch 4001/10000, Error: [[-0.00432991]\n",
            " [ 0.12790475]\n",
            " [ 0.10735135]\n",
            " [-0.00449789]]\n",
            "Epoch 5001/10000, Error: [[-0.00362631]\n",
            " [ 0.11687673]\n",
            " [ 0.10113011]\n",
            " [-0.00385647]]\n",
            "Epoch 6001/10000, Error: [[-0.00315181]\n",
            " [ 0.10870598]\n",
            " [ 0.09634003]\n",
            " [-0.00340337]]\n",
            "Epoch 7001/10000, Error: [[-0.00280494]\n",
            " [ 0.10229064]\n",
            " [ 0.09241875]\n",
            " [-0.00306029]]\n",
            "Epoch 8001/10000, Error: [[-0.00253683]\n",
            " [ 0.0970452 ]\n",
            " [ 0.08907542]\n",
            " [-0.00278783]]\n",
            "Epoch 9001/10000, Error: [[-0.00232109]\n",
            " [ 0.09262632]\n",
            " [ 0.08614316]\n",
            " [-0.00256388]]\n",
            "/n /n XOR prediction output: [[0.00214215]\n",
            " [0.91118215]\n",
            " [0.91648137]\n",
            " [0.00237501]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "juycvxzB8nMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}